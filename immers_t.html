<!doctype html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport"
        content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
  <title>Hello WebXR!</title>

  <!-- three.js -->
  <script src="https://unpkg.com/three@0.126.0/build/three.js"></script>
  <!-- GLTFLoader -->
  <script src="https://unpkg.com/three@0.126.0/examples/js/loaders/GLTFLoader.js"></script>
</head>
<body>

<!-- Starting an immersive WebXR session requires user interaction.
    We start this one with a simple button. -->
<button id="start-btn" onclick="activateXR()">Start Hello WebXR</button>
<button id="record-btn" onclick="toggleRecording()" style="display: none; margin-top: 10px; padding: 10px 20px; font-size: 16px; background-color: #4CAF50; color: white; border: none; border-radius: 5px; cursor: pointer;">
  Начать запись
</button>
<style>
  #record-btn.recording {
    background-color: #f44336;
    animation: pulse 1.5s infinite;
  }
  @keyframes pulse {
    0%, 100% { opacity: 1; }
    50% { opacity: 0.7; }
  }
  button {
    padding: 10px 20px;
    font-size: 16px;
    margin: 5px;
    cursor: pointer;
  }
</style>
<script>
// Variables for video recording
let isRecording = false;
let mediaRecorder = null;
let recordedChunks = [];
let recordingCanvas = null;
let recordingCtx = null;
let videoStream = null;
let session = null;
let renderer = null;
let gl = null;

async function activateXR() {
  if (navigator.xr) {
    alert("Supports XR!");
  } else {
    alert("No XR Support(");
  }
  
  // Hide start button
  document.getElementById('start-btn').style.display = 'none';
  
  // Add a canvas element and initialize a WebGL context that is compatible with WebXR.
  const canvas = document.createElement("canvas");
  document.body.appendChild(canvas);
  gl = canvas.getContext("webgl", {xrCompatible: true});

  const scene = new THREE.Scene();

  // Add lighting to the scene
  // Hemisphere light for ambient lighting (sky and ground colors)
  const hemisphereLight = new THREE.HemisphereLight(0xffffff, 0x444444, 1);
  hemisphereLight.position.set(0, 1, 0);
  scene.add(hemisphereLight);
  
  // Directional light for main illumination
  const directionalLight = new THREE.DirectionalLight(0xffffff, 0.8);
  directionalLight.position.set(1, 1, 1);
  scene.add(directionalLight);
  
  // Additional ambient light for better visibility
  const ambientLight = new THREE.AmbientLight(0xffffff, 0.5);
  scene.add(ambientLight);

  // Load GLTF model
  const loader = new THREE.GLTFLoader();
  let model = null;
  
  try {
    const gltf = await new Promise((resolve, reject) => {
      loader.load(
        './horse_gltf/Horse_animation.gltf',
        (gltf) => resolve(gltf),
        (progress) => console.log('Loading progress:', progress),
        (error) => reject(error)
      );
    });
    
    model = gltf.scene;
    
    // Position the model in front of the camera (negative Z is forward in AR)
    model.position.set(0, 0, -0.5);
    
    // Scale the model if needed (adjust based on model size)
    model.scale.set(0.07, 0.07, 0.07);
    
    scene.add(model);
    console.log("GLTF model loaded successfully");
  } catch (error) {
    console.error("Error loading GLTF model:", error);
    alert("Error loading GLTF model:" + error);
    // Fallback: create a simple cube if model fails to load
    const materials = [
      new THREE.MeshBasicMaterial({color: 0xff0000}),
      new THREE.MeshBasicMaterial({color: 0x0000ff}), 
      new THREE.MeshBasicMaterial({color: 0x00ff00}),
      new THREE.MeshBasicMaterial({color: 0xff00ff}),
      new THREE.MeshBasicMaterial({color: 0x00ffff}),
      new THREE.MeshBasicMaterial({color: 0xffff00})
    ];
    const cube = new THREE.Mesh(new THREE.BoxBufferGeometry(0.2, 0.2, 0.2), materials);
    cube.position.set(0, 0, -0.5);
    scene.add(cube);
  }

  // Set up the WebGLRenderer, which handles rendering to the session's base layer.
  renderer = new THREE.WebGLRenderer({
    alpha: true,
    preserveDrawingBuffer: true,
    canvas: canvas,
    context: gl
  });
  renderer.autoClear = false;
  
  // The API directly updates the camera matrices.
  // Disable matrix auto updates so three.js doesn't attempt
  // to handle the matrices independently.
  const camera = new THREE.PerspectiveCamera();
  camera.matrixAutoUpdate = false;

  console.log("Run immersive session");

  // Initialize a WebXR session using "immersive-ar".
  session = await navigator.xr.requestSession("immersive-ar");
  session.updateRenderState({
    baseLayer: new XRWebGLLayer(session, gl)
  });
  
  // Show record button
  document.getElementById('record-btn').style.display = 'block';
  
  // Setup video recording
  setupVideoRecording();
  
  // A 'local' reference space has a native origin that is located
  // near the viewer's position at the time the session was created.
  const referenceSpace = await session.requestReferenceSpace('local');

  // Create a render loop that allows us to draw on the AR view.
  const onXRFrame = (time, frame) => {
    // Queue up the next draw request.
    session.requestAnimationFrame(onXRFrame);
  
    // Bind the graphics framebuffer to the baseLayer's framebuffer
    gl.bindFramebuffer(gl.FRAMEBUFFER, session.renderState.baseLayer.framebuffer)
  
    // Retrieve the pose of the device.
    // XRFrame.getViewerPose can return null while the session attempts to establish tracking.
    const pose = frame.getViewerPose(referenceSpace);
    if (pose) {
      // Clear the framebuffer before rendering
      gl.clear(gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT);
      
      // Render each view (usually one for mobile AR)
      for (const view of pose.views) {
        const viewport = session.renderState.baseLayer.getViewport(view);
        gl.viewport(viewport.x, viewport.y, viewport.width, viewport.height);
        
        renderer.setSize(viewport.width, viewport.height, false);
  
        // Use the view's transform matrix and projection matrix to configure the THREE.camera.
        camera.matrix.fromArray(view.transform.matrix);
        camera.projectionMatrix.fromArray(view.projectionMatrix);
        camera.updateMatrixWorld(true);
  
        // Render the scene with THREE.WebGLRenderer.
        renderer.render(scene, camera);
      }
      
      // Copy frame to recording canvas if recording
      if (isRecording) {
        copyFrameToRecordingCanvas();
      }
    }
  }
  session.requestAnimationFrame(onXRFrame);
}

// Setup video recording using canvas capture
function setupVideoRecording() {
  // Create a canvas for recording
  recordingCanvas = document.createElement('canvas');
  recordingCanvas.width = window.innerWidth;
  recordingCanvas.height = window.innerHeight;
  recordingCanvas.style.display = 'none';
  document.body.appendChild(recordingCanvas);
  recordingCtx = recordingCanvas.getContext('2d');
  
  // Get canvas stream for recording
  videoStream = recordingCanvas.captureStream(30); // 30 FPS
}

// Function to copy frame from WebGL framebuffer to recording canvas
function copyFrameToRecordingCanvas() {
  if (!recordingCanvas || !recordingCtx || !renderer || !session || !isRecording) return;
  
  try {
    // Get viewport dimensions from the current view
    const baseLayer = session.renderState.baseLayer;
    if (!baseLayer) return;
    
    // Use a reasonable size for recording
    const width = recordingCanvas.width || window.innerWidth;
    const height = recordingCanvas.height || window.innerHeight;
    
    // Read pixels from the current framebuffer (baseLayer)
    // Note: We need to read from the bound framebuffer
    const pixels = new Uint8Array(width * height * 4);
    
    // Bind the baseLayer framebuffer to read from it
    gl.bindFramebuffer(gl.READ_FRAMEBUFFER, baseLayer.framebuffer);
    gl.readPixels(0, 0, width, height, gl.RGBA, gl.UNSIGNED_BYTE, pixels);
    
    // Flip vertically (WebGL has origin at bottom-left, canvas at top-left)
    const flippedPixels = new Uint8Array(width * height * 4);
    for (let y = 0; y < height; y++) {
      const srcRow = (height - 1 - y) * width * 4;
      const dstRow = y * width * 4;
      flippedPixels.set(pixels.subarray(srcRow, srcRow + width * 4), dstRow);
    }
    
    // Create ImageData and put it on canvas
    const imageData = new ImageData(new Uint8ClampedArray(flippedPixels), width, height);
    recordingCtx.putImageData(imageData, 0, 0);
  } catch (error) {
    console.error("Error copying frame:", error);
  }
}

// Toggle recording function
function toggleRecording() {
  if (!session || !videoStream) {
    alert("AR session not started yet");
    return;
  }
  
  if (!isRecording) {
    // Start recording
    startRecording();
  } else {
    // Stop recording
    stopRecording();
  }
}

function startRecording() {
  if (mediaRecorder && mediaRecorder.state !== 'inactive') {
    return;
  }
  
  // Determine the best video codec
  let options = { mimeType: 'video/webm;codecs=vp9' };
  if (MediaRecorder.isTypeSupported('video/mp4')) {
    options = { mimeType: 'video/mp4' };
  } else if (MediaRecorder.isTypeSupported('video/webm;codecs=vp9')) {
    options = { mimeType: 'video/webm;codecs=vp9' };
  } else if (MediaRecorder.isTypeSupported('video/webm;codecs=vp8')) {
    options = { mimeType: 'video/webm;codecs=vp8' };
  } else {
    options = { mimeType: 'video/webm' };
  }
  
  try {
    mediaRecorder = new MediaRecorder(videoStream, options);
    recordedChunks = [];
    
    mediaRecorder.ondataavailable = (event) => {
      if (event.data && event.data.size > 0) {
        recordedChunks.push(event.data);
      }
    };
    
    mediaRecorder.onstop = () => {
      if (recordedChunks.length === 0) {
        alert("No video data recorded");
        return;
      }
      
      const blob = new Blob(recordedChunks, { 
        type: options.mimeType || 'video/webm' 
      });
      
      // Create download link
      const url = URL.createObjectURL(blob);
      const a = document.createElement('a');
      a.href = url;
      a.download = `ar-recording-${Date.now()}.${options.mimeType.includes('mp4') ? 'mp4' : 'webm'}`;
      document.body.appendChild(a);
      a.click();
      document.body.removeChild(a);
      URL.revokeObjectURL(url);
      
      recordedChunks = [];
      alert("Video saved!");
    };
    
    mediaRecorder.onerror = (event) => {
      console.error("MediaRecorder error:", event);
      alert("Recording error: " + event.error);
    };
    
    mediaRecorder.start(1000); // Collect data every second
    isRecording = true;
    const recordBtn = document.getElementById('record-btn');
    recordBtn.textContent = 'Остановить запись';
    recordBtn.classList.add('recording');
    
    console.log("Recording started");
  } catch (error) {
    console.error("Error starting recording:", error);
    alert("Error starting recording: " + error.message);
  }
}

function stopRecording() {
  if (mediaRecorder && mediaRecorder.state !== 'inactive') {
    mediaRecorder.stop();
    isRecording = false;
    const recordBtn = document.getElementById('record-btn');
    recordBtn.textContent = 'Начать запись';
    recordBtn.classList.remove('recording');
    console.log("Recording stopped");
  }
}
</script>
</body>
</html>
